# Overview
Deploy and run large language models (LLMs) locally on Kubernetes cluster using Ollama. 

### 1. Deploy Ollama on Kubernetes
[ollama_on_kubernetes](https://github.com/vineethac/Ollama/tree/main/ollama_on_kubernetes)

### 2. Prompt LLMs using Ollama, LangChain and Python
[ollama_langchain](https://github.com/vineethac/Ollama/tree/main/ollama_langchain)


### 3. Deploy Ollama Web UI on Kubernetes
[ollama_webui](https://github.com/vineethac/Ollama/tree/main/ollama_webui)

### 4. Prompting LLaVA with image using Ollama and Python
[ollama_vision_assistant](https://github.com/vineethac/Ollama/tree/main/ollama_vision_assistant)
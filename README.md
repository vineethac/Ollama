# Overview
Deploy and run large language models (LLMs) locally on Kubernetes cluster using Ollama. 

### 1. Deploy Ollama on Kubernetes
[ollama_on_kubernetes](https://github.com/vineethac/Ollama/tree/main/ollama_on_kubernetes)

### 2. Prompt Ollama/ LLM models using LangChain and Python
[ollama_langchain](https://github.com/vineethac/Ollama/tree/main/ollama_langchain)

### 3. Deploy Ollama Web UI on Kubernetes
[ollama_webui](https://github.com/vineethac/Ollama/tree/main/ollama_webui)